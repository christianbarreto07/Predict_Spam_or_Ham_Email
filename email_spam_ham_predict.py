# -*- coding: utf-8 -*-
"""Email_SPAM_Detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10P7aTRAfQTPRXSVIDybSVU5bcYYVhOOH

Carregamos os dados.
"""

import pandas as pd
import numpy as np

data_spam_ham = pd.read_csv('/content/spam_ham_dataset.csv') #Coloco os dados em uma variável e controlo o formato do arquivo com o index_col=[0]

"""Removendo coluna Unnamed: 0 no próprio data_spam_ham"""

data_spam_ham.drop(['Unnamed: 0'], axis=1, inplace=True)

"""Impressão dos dados"""

#data_spam_ham

"""Analisando o dataset."""

#np.unique(data_spam_ham['label_num'], return_counts = True) # 3672 emails legítimos e 1499 spams.

#data_spam_ham[data_spam_ham['label_num'] == 0] #Emails Legítimos.

#data_spam_ham[data_spam_ham['label_num'] == 1] #Emails spam.

"""Importando biblioteca para trabalhar com linguagem natural(Humana) e para extração de informações de texto."""

import nltk

"""Stopwords é um conjunto de palavras que não agregam conhecimento, ou seja, não farão sentidos na analise e deixará o processamento mais custoso. Ex: preposições."""

from nltk.corpus import stopwords

"""Strings para trabalhar com funções e cadeia de palavras."""

import string

"""Importando Sklearn para trabalharmos com o aprendizado de máquina."""

from sklearn.model_selection import train_test_split

from sklearn.pipeline import Pipeline

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.naive_bayes import MultinomialNB

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

Fazendo download dos pacotes do nltk. (Uma unica vez é necessária chamar essa função)

#nltk.download())
#nltk.download('stopwords')

"""Criando nova coluna chamada length (Comprimento do subject)"""

data_spam_ham['length'] = data_spam_ham['text'].apply(len)
#data_spam_ham['length']

"""Analisando o tamanho das mensagens pelo tipo (spam ou ham)."""

data_spam_ham.hist(column='length', by='label_num', bins=70,figsize=(15,6)) #Bins para o fator de agrupamento das informações. Figsize para o tamanho

"""Criando uma função para processar o texto"""

def processaTexto(texto):
  nopunc = [char for char in texto if char not in string.punctuation] #Remoção de pontuações
  nopunc = ''.join(nopunc) #agrupando o que foi desagrupado com o nopunc.
  CleanWords = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')] #Removendo palavras desnecessárias, separando as strings em sub-strings e transformando todos em letras minúsculas.
  return CleanWords

"""Aplicando a função processaTexto nos textos do dataset e processando todos os dados."""

data_spam_ham['text'].apply(processaTexto)

"""Separando os previsores e classe em variáveis."""

X_spam_ham = data_spam_ham.iloc[:, 1].values #Previsores
#X_spam_ham

Y_spam_ham = data_spam_ham.iloc[:,2].values #Classe
#Y_spam_ham # 0 = Ham(Legitimo), 1 = Spam

"""Criando as variáveis de treino e teste para previsores e classe."""

X_data_spam_ham_treino, X_data_spam_ham_teste, Y_data_spam_ham_treino, Y_data_spam_ham_teste = train_test_split(X_spam_ham, Y_spam_ham, test_size=0.15, random_state=0)
#Criei as variáveis, chamei a função de train_test_split, passei os paramentros de previsores(x) e classe(y) referente ao dataset, coloquei 15% do dataset para teste e o restante para treino.

"""Transferindo esses novos conjuntos para o naive bayes usando o conceito de pipeline"""

pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=processaTexto)), #Contando o Bag Of Words(Saco de palavras) para definir as palavras que são tipicas de spam ou as típicas de ham.
    ('tfidf', TfidfTransformer()), #Faz com que pegue as palavras que aparecem nos dois conjuntos(ham,spam) não seja tão levada em consideração para a analise final e balanceando essas palavras.
    ('classifier', MultinomialNB()) #Aplicando o treinamento de NaiveBayes
])

"""Começando o treinamento"""

pipeline.fit(X_data_spam_ham_treino, Y_data_spam_ham_treino) #chamando a função para o treinamento e passando como parametro as variáveis X(previsores) e Y(classe) de treino.

predict = pipeline.predict(X_data_spam_ham_teste) #Fazendo a predição

"""Informações detalhadas sobre os resultados."""

accuracy_score(Y_data_spam_ham_teste, predict) #Checando a porcentagem de acerto

classification_report(Y_data_spam_ham_teste, predict) #Mostrando os valores do precision, recall, f1-score, support e accuracy.

confusion_matrix(Y_data_spam_ham_teste, predict)
#Matriz de confusão na qual mostra os verdadeiros posivos e negativos, juntamente com os falsos positivos e negativos
